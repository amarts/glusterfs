----- BEGIN -----

23:31 < k3y> cool
23:31 < k3y> ok guys, i think glusterfs design has slowly grown big
23:32 < Dyno> ..
23:32 < k3y> and we really need to sync up, have a discussion about the design from scratch
23:32 < k3y> so that all of us have a clear understnading
23:32 < k3y> moving forward, we will have to make glusterfs 'enterprise class ready'
23:33 < k3y> without which, we will not earn business
23:33 < Dyno> ...
23:33 < k3y> having a clear understanding of the big picture is absolutely necessary for _all_ of us
23:33 < k3y> the 2.2 branch was started for performance and stability
23:34 < k3y> (stability here is not about segfaults and memror leaks, but being resistant to 
             bringing down nodes randomly, killing processes randomly)
23:34 < Dyno> ...
23:34 < k3y> if we are not in sync about the design, there is no way we can reach this kind of 
             stabilty
23:35 < benki> ..
23:35 < k3y> segfaults and memleaks are out of question, i assume we are way mature more than having 
             such probs
23:35 < bulde_> hmm
23:35 < bulde_> true
23:35 < k3y> we will keep hitting them in the migration
23:35 < k3y> till the migration gets stabilized
23:35 < k3y> but that's ok
23:35 < k3y> we can solve them
23:35 < benki> ...
23:36 < Dyno> whether we are mature enough is debatable, but we can't have segfaults :P
23:36 < bulde_> Dyno: sari pa
23:36 < k3y> dyno - absolutely. what i meant was we are mature enough to handle them, and not 
             clueless why things are segfaulting
23:36 < Dyno> ...
23:36 < k3y> but a more important thing is, we are now asynchronous in the new model
23:37 < k3y> transactions no more happen in a C flow of control
23:37 < bulde_> right
23:37 < k3y> we pause things, continue them later
23:37 < k3y> it is very very vulnerable for us to lose control here
23:37 < k3y> bugs will result in unexpected results, not segfaults or leaks
23:38 < k3y> we _need_ to get a crystal clear of the design.
23:38 < benki> exactly
23:38 < k3y> we must perfectly undrstand our transaction call convention, and a lot more things
23:38 < k3y> i want to start a from scratch explaination of the design again
23:38 < k3y> which you guys already know
23:39 < k3y> but it is important to keep hearing it again and again
23:39 < k3y> and we dont one day realize 'oh, why is this piece of code existing at all? how does it 
             make a difference?'
23:39 < k3y> ok that's for intro ;)
23:40 < Dyno> yes
23:40 < k3y> the previous fuse model was synchronous
23:40 < k3y> libfuse had the main loop
23:40 < k3y> the main loop was around polling on /dev/fuse
23:40 < k3y> it would spawn us a thread per transaction
23:40 < k3y> and we had to return the thread with the results in hand
23:41 < benki> ...
23:41 < Dyno> thread/transaction??
23:41 < k3y> we were 'on contract' with libfuse 
23:41 < k3y> thread:transaction was 1;1
23:41 < Dyno> oh
23:41 < k3y> 1 thread per transaction
23:41 < Dyno> ok
23:41 < k3y> libfuse would give us a job, and we would finish *that job* only
23:42 < k3y> that surely was not the way to scale
23:42 < k3y> we had to 'block' till reply comes
23:42 < k3y> and socket transactions were synchronous
23:42 < bulde_> ...
23:42 < k3y> the thread woudl wait *at the socket* till the reply comes
23:43 < k3y> and if we had to send two prallel network transactions, we had to make it two threads
23:43 < k3y> definitely not scalable
23:43 < Dyno> and the user would be blocked inside the kernel waiting on /dev/fuse?
23:43 < k3y> right, user is blocked till our 'contract thread' returns (after which reply was 
             written to /dev/fuse)
23:43 < bulde_> k3y: wait
23:43 < k3y> the kernel thread pics the reply and returns the user's syscll
23:43 < k3y> bulde_ yes
23:44 < bulde_> make sure this discussion is logged somewhere
23:44 < bulde_> some one please..
23:44 < bulde_> or.. let me do it
23:44 < Dyno> ok
23:44  * bulde_ takes log of this
23:44 < bulde_> k3y: continue
23:44 < k3y> if you are logging it from bitchx's log command, stuff before the command will not be 
             logged
23:45 < k3y> continue?
23:45 < bulde_> yes
23:45 < k3y> ok
23:46 < k3y> the user's syscall waits immaterial of what model we use in libfuse
23:46 < k3y> what matters for user is when the reply is written back to /dev/fuse
23:46 < Dyno> yes
23:46 < k3y> if having to work on contract within a thread is not working for us, we throw it away 
             and implment a model which works of us, as long as we ensur we are writing back the 
             reply to /dev/fuse
23:47 < bulde_> coorect
23:47 < k3y> so now we have the asynchronous model
23:47 < k3y> communication between server and client is in terms of transactions
23:47 < k3y> every transaction has a 'callid'
23:47 < k3y> we send a request with a callid
23:47 < k3y> and the reply comes for that callid asynchronously, sometime later
23:48 < k3y> the transaction is put on pause, and we resume the transaction when it comes later
23:48 < k3y> this calls for two requirements - asynchronous networking, saving transaction state to 
             be resumable later
23:48 < k3y> hence the two major changes in glusterfs - abstraction of transport layer, introduction 
             of STACK_WIND/UNWIND with seperate callbacks
23:49 < k3y> we stick with the translator model.. that had no problems, and that was good
23:49 < k3y> but we rewrite them to suit the asynchronous transaction model, by breaking every 
             function into a request and an asynchronous reply callback
23:49 < benki> ....
23:50 < k3y> first i'll talk about the STACK
23:50 < oscarW1ld3> ..
23:50 < k3y> the C flow of calls depends on the stack
23:50 < k3y> state of the thread is represented in the stack
23:50 < k3y> where to return
23:50 < k3y> local variables
23:50 < bulde_> ...
23:51 < k3y> and if we have to pause the thread and do something else, the stack has to be unwound
23:51 < k3y> before this
23:51 < k3y> let me clear the terms WIND and UNWIND
23:51 < k3y> think of a stick, and a thread being wound around it
23:52 < k3y> making an extra call, is like winding the thread one more round
23:52 < k3y> so WIND == make a new call (preserving all the state needed for return and reume)
23:52 < k3y> UNWIND = return from current frame back to previous frame
23:52 < k3y> each WIND adds a new frame
23:52 < k3y> 1 frame is for 1 function call
23:52 < Dyno> ...
23:52 < k3y> clear?
23:53 < k3y> ...
23:53  * bulde_ understands
23:53 < k3y> benki / oscarW1ld3 ?
23:53 < benki> ya
23:53 < k3y> this is very important to have the clear picture here
23:53 < oscarW1ld3> go ahead
23:53 < k3y> ok cool
23:53  * Dyno t
23:54 < k3y> our limitation is, having translators in C, returning from function implies we are 
             'done' with
23:54 < k3y> but we are actually 'done with' when reply comes back from network
23:54 < k3y> and also we dont want to not-return till reply comes
23:54 < Dyno> yes
23:54 < bulde_> exactly
23:55 < k3y> the solution is, implment translators in a language a layer above C
23:55 < k3y> where C return is not translator return
23:55 < Dyno> in one sentence, our new philosophy is: "Everything that can proceed must proceed"
23:55 < k3y> dyno - right
23:55 < k3y> that's where we bring in the new virtual call stack, implemented on the heap
23:56 < k3y> if C function returns, it does not imply translator call has returned
23:56 < k3y> makeing a C call does not imply we have made a call in our virtual stack
23:56 < Dyno> yes
23:56 < benki> :?
23:56 < k3y> the new call convention is to implment a poor man's continuation mechanism
23:57 < bulde_> jai lisp
23:57 < Dyno> resumable-one-time-only continuations
23:57 < k3y> benki should i explain something more?
23:57 < benki> no... continue... 
23:57 < benki> will try thinking on own and understand
23:57 < benki> tale swalpa rust hiDididde
23:57 < k3y> if something is not clear, make sure the cloud of doubt doesnt grow
23:58 < bulde_> benki: true, don't keep doubts, even it may be silly
23:58 < k3y> to make a call from one translator to another translator, we do not want to use the C 
             calls because we are forced to return with result. So inter-trnaslator calls are made 
             using our virtual stack
23:59 < Dyno> if we ever write a paper on glusterfs it's going to look good :D
23:59 < k3y> there are two fundamental primitives
23:59 < k3y> STACK_WIND, STACK_UNWIND
23:59 < k3y> Dyno yes it will :p
23:59 < k3y> STACK_WIND is similar to : function(args)
Day changed to 05 Nov 2006
00:00 < k3y> STACK_UNWIND is ismilar to: return ret;
00:00 < Dyno> return (args, ...)
00:00 < k3y> yes, return (args,...) is more correct
00:00 < k3y> these primitives are also suitable for object oriented programming
00:00 < k3y> because they also have the 'implicit object' for each frame
00:01 < Dyno> yes, please explain the 'implicit object' concept
00:01 < k3y> ok
00:01 < k3y> think C++
00:01 < benki> ...
00:01 < k3y> any thread is happening only within a method of an object
00:01 < Dyno> ...
00:02 < k3y> there is no flow of control outside an object
00:02 < Dyno> 'active object'
00:02 < Dyno> ok
00:02 < k3y> you dont have a random function executing outside the context of any object
00:02 < k3y> the entire world is only object
00:02 < k3y> the entire world is only objects
00:02 < k3y> and actions happen within objects
00:02 < Dyno> ...
00:03 < k3y> there is no action outside an object.. ok that's for the explaination
00:03 < k3y> our translator is a graph of ojects
00:03 < bulde_> ...
00:03 < k3y> each volume in the tree (no more :D) is an object
00:03 < Dyno> :PPP
00:03 < k3y> they belong to a class (specified by 'type optimizer/stat-prefetch' etc)
00:03 < k3y> and have local variables and state
00:03 < Dyno> ashte ashte ... we have our own object system and stack
00:04 < bulde_> NEW LANG?
00:04 < oscarW1ld3> :O
00:04 < k3y> :P
00:04 < bulde_> :D
00:04 < k3y> so flow of control jumps from object to object
00:04 < Dyno> (xlator_t *) is the instance
00:04  * oscarW1ld3 shivers
00:04 < bulde_> paper on this will rock
00:04 < k3y> from trnaslator to translator
00:04 < Dyno> k3y: 'state' of instance being?
00:04 < k3y> inter-translator call means inter-object call
00:05 < Dyno> the local vars are stored in the call_frame_t, right?
00:05 < k3y> that is the variables on the stack (virutal stack)
00:05  * benki will take manegerial role and order k3y to write a paper soon
00:05 < k3y> there are 'global variables' within an object
00:05 < Dyno> where are they stored?
00:06 < Dyno> priv?
00:06 < k3y> liek how yo uuse this->private
00:06 < k3y> xl->private
00:06 < Dyno> ok
00:06 < bulde_> local
00:06 < benki> is object also a procedure??
00:06 < k3y> that is like private: variables in C++
00:06 < k3y> object is an object :) like C++ object
00:06 < Dyno> okpa
00:06 < benki> :(
00:06 < k3y> here xlator_t *
00:06  * Dyno sees a future PL designer in k3y 
00:06 < k3y> object implmeents methods, which are our fops
00:06  * benki doesn't like k3y's attitude now.. object cannot be an object, it has to be a procedure
00:07 < k3y> benki :O
00:07  * k3y a w benki
00:07  * Dyno t
00:07 < k3y> ok object is a procedure
00:07 < benki> :)
00:07  * bulde_ :O
00:07 < Dyno> ok, methods
00:07 < k3y> now, if we have to make a 'call' in _our language_, from one method to another, we have 
             to use STACK_WIND
00:07 < bulde_> continue
00:08 < Dyno> if we write a static-type checker for our language it would rock
00:08 < k3y> Dyno we need that.. i'm already seen a bad need for it :|
00:08  * Dyno will write it in haskell :P
00:08 < k3y> remember, our 'functions' are not C functions
00:09 < k3y> do _not_ relate our function to C functions
00:09 < Dyno> eh?
00:09 < Dyno> "our" methods are ultimately C functions
00:09 < k3y> in C, when you make a call, the code after the return is in the same C procedure
00:09 < Dyno> hange
00:09 < k3y> but for us, when you return, we continue in a differnt C procedure
00:09 < bulde_> ashte ashte
00:09 < Dyno> we pass the continuation for the function call explicitly
00:10 < Dyno> CPS
00:10 < benki> bOLi makLaa
00:10 < k3y> in C procedure, local variables are on the stack, accessible as variables
00:10 < k3y> our local variables are also in our frame, accessiable as frame->local
00:10 < k3y> in C, local variables are accessible before and after a function call from within
00:10 < oscarW1ld3> bit bidro
00:11 < oscarW1ld3> :O
00:11 < k3y> in our 'lang', frame->local is accessible before and after you STACK_WIND and _UNWIND 
             from a differnt procedure
00:11 < k3y> is this understood?
00:11 < Dyno> yep
00:11 < k3y> bulde_ ?
00:11 < bulde_> yep
00:11 < k3y> benki ?
00:11 < k3y> oscarW1ld3 ?
00:11 < benki> eega ishTu saaku
00:11 < oscarW1ld3> come again
00:12 < bulde_> i use frame->local extensively
00:12 < benki> i seriously need some time to digest this research
00:12  * oscarW1ld3 t
00:12 < bulde_> benki: the code is already there
00:12 < bulde_> :p
00:12 < k3y> oscarW1ld3 we have a virtual call stack, where each function in our stack is more than 
             on C function
00:12 < bulde_> k3y: can i tell?
00:12 < benki> bulde_: i need to go through the code myself and make sure i actually understand this 
               concept
00:12 < k3y> all C functions within one call frame share the same frame->local pointer
00:13 < k3y> bulde_ yep
00:13 < bulde_> oscarW1ld3: see this code
00:13 < bulde_> read () {
00:13 < bulde_> int32_t ret = 0;
00:13 < k3y> dyno -
00:13 < bulde_> xl->fops->read ()
00:14 < bulde_> return ret;
00:14 < bulde_> }
00:14 < k3y> our continuations are resumable multiple times
00:14 < k3y> Dyno - continuation is preserved till you explictly call STACK_DESTROY
00:14 < bulde_> now, in new _lang_
00:14 < bulde_> the read itself is split
00:14 < Dyno> k3y: hmm
00:14 < oscarW1ld3> bulde_ ok
00:14 < k3y> you can 'continue' from *any* frame in the saved continuation, any number of times
00:15 < bulde_> k3y: Dyno, till that frame exists
00:15 < bulde_> ie, if you don't free it
00:15 < k3y> all frames exist till STACK-DESTROY is done
00:15 < bulde_> ok
00:15 < k3y> ok, lets continue
00:15 < Dyno> ...
00:15 < bulde_> oscarW1ld3: can you see any xlator now ?
00:15 < bulde_> preferably, unify :p
00:15 < oscarW1ld3> yes 
00:16 < Dyno> oscarW1ld3: preferably, not
00:16 < oscarW1ld3> what?
00:16 < bulde_> :D
00:16 < k3y> whenever i make a call in C++, i have to do: obj.method (args)
00:16 < k3y> oscarW1ld3 see defults.c
00:16 < Dyno> ...
00:16 < k3y> defaults.c
00:16 < oscarW1ld3> yep
00:16 < k3y> whenever i make a call in C++, i have to do: obj.method (args)
00:16 < k3y> the return point is implicit by the location of the code
00:16 < k3y> in the new method i do
00:17 < k3y> STACK_WIND (current_frame, ret_fn, obj, method, args)
00:17 < k3y> in C++ current frame and return point are implicit
00:17 < k3y> for us, it is explicit
00:17 < oscarW1ld3> ........
00:17 < Dyno> ok
00:18 < bulde_> hmm
00:18 < k3y> once the STACK is wound, it remains in that state till *somebody* calls STACK_UNWIND
00:18 -!- DynWind [i=vikas@80x25.org] has quit [Nick collision from services.]
00:18 -!- Dyno is now known as DynWind
00:18 < k3y> it does not matter which C function calls the STACK_UNWIND
00:18 -!- DynWind_ [i=vikas@80x25.org] has joined #zr
00:19 < bulde_> right
00:19 < k3y> suppose the C function which you STACK_WIND'ed into returns, the virtual stack is still 
             there as-is on the heap
00:19 < k3y> you can STACK_UNWIND from the frame you last left anytime later, from any C function
00:19 < k3y> this is exactly our requirement
00:20 < k3y> we keep STACK_WIND'ing into objects
00:20  * bulde_ claps for a neat and elegant design by k3y
00:20  * oscarW1ld3 applauds
00:20 < k3y> and 'jump out' from there (escaping with C return's)
00:20  * benki cries
00:20  * oscarW1ld3 slaps benki 
00:20 < k3y> and later when asynchronous reply comes, i STACK_UNWIND from where we had left out
00:20 < benki> :'(
00:20 < k3y> its not my design, just implmented continuations from lisp :p
00:21 < oscarW1ld3> k3y how is this different from context switching?
00:21  * DynWind gets ideas of glusterfs-over-jabber
00:22 < k3y> the STACK_UNWINDING happens, exactly returning from the way they were STACK-WINDed
00:22 < k3y> oscarW1ld3 it is context switching, we keep switching between transaction contexts
00:22 < oscarW1ld3> ..
00:22 < k3y> dynwind jabber later pleeeeaze :D
00:22 < k3y> not now
00:22 < k3y> now glusterfs
00:23 < k3y> another _very_ important things
00:23 < k3y> our STACK is not single threaded
00:23 < k3y> it is multi threaded
00:23 < k3y> exactly what we wanted for unify
00:23 < bulde_> right
00:23 < k3y> flows into child nodes should happen parallely
00:24 < k3y> when you STACK-WIND into a child node,
00:24 < DynWind> ...
00:24 < bulde_> ...
00:24 < k3y> when you are done for now, you can voluntarily give up by returning the C function so 
             that unify can parallely STACK_WIND into another child
00:24 < k3y> so in a given STACK context, there can be multiple threads
00:24 < bulde_> like gping 
00:25 < k3y> and they are all co-oepartive
00:25 < DynWind> how will unify know which child returned?
00:25 < DynWind> due to UNWIND?
00:25 < k3y> dynwind - that is a thing which we need to enhance yet
00:25 < k3y> it does not know yet :(
00:25 < bulde_> DynWind: still under progress
00:26 < bulde_> DynWind: thats verymuch required for oen
00:26 < bulde_> *open
00:26 < k3y> think like this -
00:26 < DynWind> wait
00:26 < k3y> pthread_create ()
00:26 < DynWind> ...
00:26 < k3y> pthread_create ()
00:26 < k3y> multiple of them
00:26 < DynWind> ok
00:26 < k3y> later pthread_wait_for_join()
00:26 < k3y> each thread falls back
00:26 < k3y> and you are waiting
00:26 < DynWind> question
00:26 < k3y> yes ask
00:27 < DynWind> you do WIND() on one child, and another frame is added to the stack. If you do 
                 WIND() again on that stack, won't the new frame be added as child of the prev 
                 (first WIND's) frame?
00:27 < bulde_> dyn, its not recursive
00:28 < bulde_> ie, frame remains same
00:28 < DynWind> ok
00:28 < dtor> k3y is hung
00:28 < bulde_> :O
00:28 < DynWind> this is like multiple threads with a *single* stack
00:29 < dtor> ok i'll explain
00:29 < dtor> its like a tree
00:29 < DynWind> whereas in pthread each thread gets its own stack
00:29 < dtor> each frame has his parent frame pointer
00:29 < DynWind> ...
00:29 < dtor> when you STACK_WIND, you create a new frame and make current frame its parent (needed 
              for STACK_WIND)
00:29 < dtor> and you dont care about other frams which already have this as parent
00:30 < DynWind> ah ... so the stack *branches*?
00:30 < DynWind> hehehe
00:30 < dtor> yes, stack branches :D
00:30 < DynWind> ashte ashte!
00:30 < dtor> it was exactly tailor made for our needs
00:30 < dtor> we needed branching of flow at unify
00:31 < dtor> when each child brnach UNWIND's
00:31 < dtor> it finishes its sub-loop
00:31 < dtor> and when all children are unwound, we can unwind back from unify
00:32 < dtor> we still need an enhancement (minor) about when a call is UNWINDed into a _cbk, to say 
              who was the one who returned
00:32 < dtor> i couldnt think of this in the design as i was relating to C++ calls
00:32 < dtor> anyways, we'll work on that
00:33 < dtor> ok, i think our stack design is now clear
00:33 < DynWind> ...
00:33 < oscarW1ld3> ...
00:33 < dtor> can we move on to transport objects?
00:33 < dtor> ah one more point
00:33 < bulde_> tell
00:34 < dtor> since the STACK has multiple branches, resuming any frame is a necessary feature
00:34 < dtor> returning and resuming are *the same* for us
00:35 < dtor> does not matter *where* you STACK_UNWIND is called
00:35 < dtor> so when a transaction request goes over the network, you need to relate which frame 
              sent it
00:35 < dtor> so that when the reply comes, you resume/return from that frame
00:36 < dtor> you need to know which branch of the stack sent the packet and that branch of the 
              stack has to be resumed
00:36 < dtor> that's the reason for having a 1:1 mapping between callid and frame_t
00:37 < dtor> clear?
00:37 < dtor> ...
00:38 < bulde_> you need to know which branch of the stack sent the packet and that branch of the 
00:38 < bulde_>               stack has to be resumed
00:38 < bulde_> tell it again?
00:38 < dtor> suppose you have unify
00:38 < bulde_> why because at client-proto only one packet goes right?
00:38 < dtor> and you make MULTIPLE stack_wind into differnt children
00:38 < DynWind> ok
00:38 < bulde_> and you store frame there
00:38 < dtor> each branch sends a packet itself
00:38 < dtor> all branches share the same call context
00:38 < bulde_> ok
00:39 < bulde_> hmm
00:39 < dtor> when a reply comes, w.r.t C flow, you are in the main loop
00:39 < dtor> out of the context of the call
00:39 < bulde_> hmm
00:39 < dtor> you need to realate which call's which branch has to be resumed
00:40 < bulde_> hmm
00:40 < bulde_> thats obvious
00:40 < dtor> yes, so callid:frame_t is a 1:1 mapping
00:40 < bulde_> ok ok
00:40 < dtor> ok clear?
00:40 < oscarW1ld3> ...
00:40  * bulde_ understands
00:41 < DynWind> yep
00:41 < dtor> i tink call context discussion we had enough
00:41 < dtor> move to transport_t ?
00:41 < DynWind> yep
00:41 < bulde_> ready
00:41 < oscarW1ld3> where is benki?
00:41 < dtor> benki - ping
00:41 < DynWind> parking car
00:41 < dtor> lets wait for him
00:41 < dtor> this is a fresh topic, he cn listen
00:42 < oscarW1ld3> dtor bnechmark is ready 
00:42 < bulde_> oscarW1ld3: :O
00:42 < oscarW1ld3> eh?
00:42 < dtor> oscarW1ld3 works?
00:43 < oscarW1ld3> going on. seeing
00:43 < oscarW1ld3> takes a lot of time
00:48 < DynWind> benki is back
00:48 -!- k3y [n=avati@201.208.46.3] has quit [Read error: 110 (Connection timed out)]
00:48 < benki> :O
00:48 < dtor> ok cool
00:49 < benki> ...
00:49 < dtor> now in our translator graph, the leaf nodes are network endpoints
00:49 < dtor> we initially had 'brick' there
00:49 < dtor> which used to erialize a call, send to socket
00:49 < dtor> and wait for reply
00:49 -!- DynWind [n=vikas@61.246.58.228] has quit ["leaving"]
00:49 < dtor> and return
00:49 < DynWind_> ...
00:50 < dtor> two problems were here
00:50 < DynWind_> ..
00:50 < dtor> one, there was a lot of ocde duplication between sdp and tcp
00:50 < DynWind_> (still there)
00:51 < dtor> DynWind_ it will be addressed
00:51 < dtor> two, synchronous nature of send and recv
00:51 < benki> ...
00:51 < dtor> the first step we have done is, move the protocol seriliazation out of transport (yes, 
              tcp and ibsdp has lot of code, but atleast not protocol srializing/unseriliazing)
00:52 < DynWind_> yep
00:52 < dtor> transports now have a sepreate interface (like schedulers)
00:52 < benki> ;o
00:52 < dtor> the endpoint of a graph is a protocol transltor
00:52 < dtor> which uses a transport object
00:53 < dtor> transport object represents the 'end point' of a pipe
00:53 < dtor> like a socket
00:53 < dtor> one part of the problem is sovled, where protocol code is no more duplicated
00:53 < dtor> next for asynchronous nature
00:53 < dtor> we have a main loop which polls over transport objects
00:53 < benki> ...
00:54 < dtor> transport objects are now close to the main loop
00:54 < dtor> main loop is aware of tranpsorts, polls over them, and wakes up a transport object 
              when there is activity
00:55 < dtor> every transport object 'belongs' to a protocol trnasltor
00:55 < dtor> on client side, transport:protocl is 1:1
00:55 < dtor> on server side, the topmost trnaslator has many transports (one per client)
00:56 < benki> ...
00:56 < dtor> the protocol tobject implements two interfaces
00:56 < dtor> one is the tranlator interface, for accepting calls from above
00:56 < dtor> the next is the notify interface from the transport object side
00:57 < benki> :O
00:57 < bulde_> this is client side
00:57 < dtor> the client protocl has 'request seriliaztion' in the translator interface, and 'reply 
              unserialize' in the notify interface
00:57 < bulde_> server side its reverse
00:57 < dtor> the server protocol has 'request unserialize' in notify interface, and reply serialize 
              in trnalstor's cbk interface
00:57 < benki> okpa
00:58 < dtor> so far things are straight forward
00:58 < dtor> when server unserailzes a request, it creates a new call context 9vritual stack)
00:58 < benki> yep
00:58 < dtor> and beings the call
00:58 < dtor> now glusterfs-fuse too is like a protocol-server
00:59 < dtor> it sits on top of the client side tree, accepting requests from /dev/fuse (another 
              transport interface)
00:59 < dtor> it uses libfuse's other functions to unserialize /dev/fuse serial data
00:59 < dtor> and again libfuse's other functions to serialize and write to /dev/fuse
01:00 < dtor> server-protocl and libfuse glue, both create a new call stack and being the cal flow
01:00 < dtor> and destroy the context after the last STACK_UNWIND is done
01:01 < dtor> there is a relation between STACKS and transport_t's
01:01 < dtor> more of a responsiblity actually
01:02 < dtor> in the client protocol, when a call has beenserialized and sent over the network, that 
              frame of control is 'accounted' by the transport_t
01:02 < dtor> it is saved in transport_t->xl_private
01:02 < DynWind_> ...
01:02 < dtor> think of old model
01:02 < dtor> write(fd, data);
01:02 < dtor> read(fd, data);
01:02 < dtor> if read failed, you had to return -1;
01:03 < dtor> but in asynchronous model, protocl unserializes data when arrives and figures which 
              frame had sent this packet
01:03 < dtor> now if the data is not available to be read, there are lot of frames which have sent 
              half transactions through this socket
01:04 < dtor> and all of them should get -e in read(fd, data) (in anlogy to old model)
01:04 < dtor> when socket dies, all the transactions should return assuming read(fd, data) returned 
              -1
01:04 < dtor> should i explain again?
01:05 < dtor> ???
01:05 < dtor> did i put you all to sleep :D
01:05 < oscarW1ld3> .?.?
01:05 < bulde_> i understood
01:05 < dtor> DynWind_ ? benki ?
01:06 < dtor> your router died?
01:06 < oscarW1ld3> i think it is
01:06 < DynWind_> nope
01:07 < dtor> ok
01:07 < DynWind_> although benki has fallen asleep
01:07 < dtor> when there is an exception on the socket, the incomplete CALL's should return as 
              though read(fd) returned -1
01:07 < DynWind_> ok
01:07 < dtor> for this we have the list of frame-t's which have sent requests over this socket (and 
              for which reply hasnt yet come)
01:08 < dtor> when there is exception, we loop over the saved fd's and STACK_UNWIND (-1, ENOTCONN) 
              to each of them
01:08 < dtor> when a regular normal packet comes back
01:09 < dtor> we unserialize, get the callid, and the frame_t
01:09 < dtor> and we remove teh frame_t from saved_frames
01:09 < dtor> because it got the reply, no need to account for it.. no more transport_t's 
              responsiblity
01:09 < dtor> when there is exception, we loop over the saved frames and STACK_UNWIND
01:09 < dtor>           (-1, ENOTCONN) to each of them
01:10 < dtor> (previously i said saved fds)
01:10 < DynWind_> ok
01:10 < dtor> this is one responbiilty
01:10 < dtor> the other, about open files
01:10 < dtor> whenever a file is opened, the client-protocl creates a new file_ctx_t in the 
              unserialize of a successful open()
01:11 < bulde_>  ...
01:11 < dtor> file_ctx_t is like a file descriptor
01:11 < DynWind_> ok
01:11 < dtor> each translator has his own context in each open file
01:11 < dtor> for example, unify may have the pointer to the child node where this file exists
01:12 < dtor> file-ctx_t is created on open() and destroyed on release()
01:12 < dtor> and all file opertions on open file refer this
01:12 < dtor> read() will be on file_ctx_t, not on path of the filename
01:13 < dtor> just like incomplet transactions over the transport_t, transport_ alsow knows which 
              are teh files which are open *through* it
01:13 < dtor> what should transport_t do when the peer is doscinnected?
01:14 < dtor> it cannot force to close a file, because close() should come from the application
01:14 < oscarW1ld3> ..
01:14 < dtor> what it *should* do is, return error on every file operation on that file_ctx_t 
              thereafter
01:14 < dtor> DynWind_ bulde_ is this confusing?
01:14 < DynWind_> no
01:14 < DynWind_> cleared things up
01:15 < dtor> ok
01:15 < dtor> now the file_ctx_t will have a component from the protocl trnaslstor
01:15  * DynWind_ eats mid-mid night dinner
01:15 < dtor> which will have the 'id' or 'oockie' which uniquely identifies the open fd on the 
              server side
01:16 < DynWind_> ??
01:16 < bulde_> DynWind_: no
01:16 < DynWind_> that 'cookie' is returned on open?
01:16 < dtor> if you do read(), the server should know which fd it is on
01:16 < dtor> yes, on open
01:16 < DynWind_> ok
01:16 < bulde_> dtor: no.. i implemented it now
01:17 < bulde_> dtor: ie, not confusing
01:17 < dtor> to uniquely identify the file-ctx_t on the server side (for now i htink we are using 
              the address of file_ctx_t object)
01:18 < dtor> on server side, whenever a transport_t dies, it closes() all the file_ctx_t which came 
              from that client
01:18 < dtor> on the client side, hence, we mark all the file-ctx-t from that transport_t as 'stale' 
              by delting the protocl's component from it
01:18 < dtor> deleting the protocl's component is just a way of marking it stale
01:19 < DynWind_> okpa
01:19 < DynWind_> so now i take care of all tranposrt related changes?
01:19 < DynWind_> and bulde is doing client-proto?
01:19 < dtor> right
01:20 < bulde_> DynWind_: i finished client-proto
01:20 < dtor> cool
01:20 < DynWind_> ok
01:20 < bulde_> dtor: is it the end of discussion
01:20 < bulde_> ?
01:20 < bulde_> can i post log and sleep?
01:21 < bulde_> i have to get up @ 6am tomorrow :O
01:21 < oscarW1ld3> i have results of POSIX
01:21 < bulde_> oscarW1ld3: cool
01:21 < dtor> yes, end of discussion
01:21 < dtor> the discussion was mainly to clarify the two major changes
01:21 < dtor> asynchronous call stack, and transport_t
01:21 < oscarW1ld3> dtor making stale .. i have seen the same thing over NFS
01:21 < bulde_> so as of now we are left with only one doubt
01:22 < oscarW1ld3> :O
01:22 < dtor> bulde_ ask
01:22 < DynWind_> ...
01:22 < bulde_> to make UNWIND know about who called it
01:23 < dtor> bulde_ the solution is simple, to add another parameter
01:24 < bulde_> oscarW1ld3: it didn't crash right?
01:24 < bulde_> dtor: hmm
01:24 < dtor> bulde_ i'm trying to think of a better way
01:24 < bulde_> if thats done i can take unify changes for tomorrow
01:24 < dtor> bulde_ that needs changes _everywhere_


----- END -----
